install.packages("swirl")
library(swirl)
install_from_swirl("Statistical Inference")
swirl()
(1-3/36)
deck
52
4/52
0
12/52
2/51
Library("swirl")
library("swirl")
swirl()
1.6*0.8*0.5
0.64
mypdf()
mypdf(0.2)
mypdf
?integrate
integrate(mypdf,0,1.6)
sqrt(200)
sqrt(2)
0.997 * 0.001
(1-0.985)*(1-0.001)
(0.997*0.001)/(0.997*0.001 + (1-0.985)*(1-0.001))
Library(swirl)
library(swirl)
swirl()
3.5
expect_dice
dice_high
expect_dice(dice_high)
expect_dice(dice_low)
0.5*(edh+edl)
integrate(myfunc,0,2)
spop
mean(spop)
allsam
apply(allsam,1,mean)
mean(smeans)
swirl()
0.8^5 + choose(5,4) * 0.8^4 * 0.2 + choose(5,3) * 0.8^3 * 0.2^2
?pbniom
pbinom(2,5,0.8,lower.tail=FALSE)
qnorm(0.1)
0
??qnorm
qnorm(97.5,3,2)
qnorm(.975,3,2)
3+1.96*2
pnorm(1200,1020,50)
pnorm(1200,1020,50,lower.tail=FALSE)
pnorm((1200-1020)/50,lower.tail=FALSE)
qnorm(0.75,1020,50)
0.53
0.53
?ppois
ppois(3,2.5*4)
?pbiom
?pbinom
pbinom(5,1000,0.01)
?ppois
p(5,1000*0.01)
ppois(5,1000*0.01)
dice_sqr
ex2_fair<-sum(dice_sqr*dice_fair)
ex2_fair - 3.5
ex2_fair - 3.5^2
sum(dice_sqr*dice_high) - dice_high^2
sum(dice_sqr*dice_high) - edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif)))
sd(apply(matrix(runif(10000),1000),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
library(swirl)
swirl()
coinPlot(10)
install.packages('labeling')
coinPlot(10)
coinPlot(10000)
qnorm(0.95)
0.6+c(-1,1)*qnorm(0.975)*sqrt(0.6(1-0.6)/100)
0.6+c(-1,1)*qnorm(0.975)*sqrt(0.6*(1-0.6)/100)
0.6+c(-1,1)*qnorm(0.975)*sqrt(0.6*0.4/100)
binom.test(60,100)$conf.int
mywald(0.2)
ACCompar(20)
lamb<-5/94.32
lamb+c(-1,1)*qpois(0.975)*sqrt(lamb/t)
lamb+c(-1,1)*qpois(0.975,lamb)*sqrt(lamb/t)
lamb+c(-1,1)*qpois(0.975,lamb)*sqrt(lamb)
lamb+c(-1,1)*qpois(0.975,lamb)*sqrt(lamb/94.32)
lamb+c(-1,1)*qnorm(0.975,lamb)*sqrt(lamb/94.32)
lamb+c(-1,1)*qnorm(0.975)*sqrt(lamb/94.32)
?poisson.test
poisson.test(5,94.32)$conf
?mean
?mode
mode(c(0.1,0.2,0.25,0.22))
hist(c(0.1,0.2,0.25,0.22))
median(c(0.1,0.2,0.25,0.22))
mean(c(0.1,0.2,0.25,0.22))
?density
density(c(0.1,0.2,0.25,0.22))
d<-density(c(0.1,0.2,0.25,0.22))
d$x[which.max(d$y)]
hist(c(0.1,0.2,0.25,0.22))
?var
var(c(US    3941679	10417
IN	3530879	7404))
var(c(10417/3941679,7404/3530879))
set.seed(1988)
#"NormalizedGini" is the other half of the metric. This function does most of the work, though
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
#df
df$random = (1:nrow(df))/nrow(df)
#df
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
#print(df)
return(sum(df$Gini))
}
NormalizedGini <- function(solution, submission) {
SumModelGini(solution, submission) / SumModelGini(solution, solution)
}
library(leaps)
library(dplyr)
library(caret)
setwd("/Users/swapnil.jamthe/work/Kaggle/LMG")
data<-read.csv("train.csv")
data<-select(data,-(Id))
trainingIndex<-createDataPartition(y=data$Hazard,p=0.8,list=FALSE)
trainingData<-data[trainingIndex,]
cvData<-data[-trainingIndex,]
testFinal<-read.csv("test.csv")
obs<-data.frame(attrs=integer(0),gini=numeric(0))
bestGini = 0
bestNtrees = 0
iter = 0;
library(randomForest)
#modelRf<-randomForest(Hazard~.,data=trainingData,ntree=500,importance=TRUE)
#imp<-as.data.frame(importance(modelRf))
#imp$attr<-rownames(imp)
#imp<-imp[order(imp$`%IncMSE`,decreasing = TRUE),]
#write.table(imp,file = "predictions/attrImp/imp",quote = FALSE,sep = ",",row.names = FALSE)
imp<-read.csv("predictions/attrImpBoosted/imp")
imp<-imp[order(imp$IncNodePurity,decreasing=TRUE),]
obs<-data.frame(numAttr=integer(0),gini=numeric(0))
bestGini = 0
bestNumAttr = 0
iter = 0;
topStartAttr = 15
ntree = 15000
depth=25
for(numAttr in seq(topStartAttr,32,2))
{
formulaStr = paste0("Hazard~",imp[1,"attr"])
for(i in 13:numAttr)
{
formulaStr<-paste0(formulaStr,"+",imp[i,"attr"])
}
print(formulaStr)
print(paste0("Ver 1 : Training for numAttr=",numAttr))
iter<-iter + 1
modelBoost<-gbm(as.formula(formulaStr),data = trainingData,distribution = "gaussian",n.trees = ntree,interaction.depth = depth)
predBoost<-predict(modelBoost,testFinal,n.trees = ntree)
predBoostFrame<-data.frame(Id=testFinal$Id,Hazard=predBoost)
write.table(predBoostFrame,file = paste0("predictions/attrImpBoosted/predBoost",depth,"_",ntree,"_",numAttr),quote = FALSE,sep = ",",row.names = FALSE)
predCv<-predict(modelBoost,cvData,n.trees = ntree)
g<-NormalizedGini(cvData$Hazard,predCv)
obs<-rbind(obs,data.frame(attrs=numAttr,gini=g))
print(paste0("iter=",iter, ",g=",g,",numAttr=",numAttr))
if(bestGini < g)
{
bestGini = g
bestNumAttr = numAttr
print(paste0("Best g=",g,",bestNumAttr=",numAttr))
}
}
library(gbm)
for(numAttr in seq(topStartAttr,32,2))
{
formulaStr = paste0("Hazard~",imp[1,"attr"])
for(i in 13:numAttr)
{
formulaStr<-paste0(formulaStr,"+",imp[i,"attr"])
}
print(formulaStr)
print(paste0("Ver 1 : Training for numAttr=",numAttr))
iter<-iter + 1
modelBoost<-gbm(as.formula(formulaStr),data = trainingData,distribution = "gaussian",n.trees = ntree,interaction.depth = depth)
predBoost<-predict(modelBoost,testFinal,n.trees = ntree)
predBoostFrame<-data.frame(Id=testFinal$Id,Hazard=predBoost)
write.table(predBoostFrame,file = paste0("predictions/attrImpBoosted/predBoost",depth,"_",ntree,"_",numAttr),quote = FALSE,sep = ",",row.names = FALSE)
predCv<-predict(modelBoost,cvData,n.trees = ntree)
g<-NormalizedGini(cvData$Hazard,predCv)
obs<-rbind(obs,data.frame(attrs=numAttr,gini=g))
print(paste0("iter=",iter, ",g=",g,",numAttr=",numAttr))
if(bestGini < g)
{
bestGini = g
bestNumAttr = numAttr
print(paste0("Best g=",g,",bestNumAttr=",numAttr))
}
}
View(imp)
for(numAttr in seq(topStartAttr,32,2))
{
formulaStr = paste0("Hazard~",imp[1,"attr"])
for(i in 1:numAttr)
{
formulaStr<-paste0(formulaStr,"+",imp[i,"attr"])
}
print(formulaStr)
print(paste0("Ver 1 : Training for numAttr=",numAttr))
iter<-iter + 1
modelBoost<-gbm(as.formula(formulaStr),data = trainingData,distribution = "gaussian",n.trees = ntree,interaction.depth = depth)
predBoost<-predict(modelBoost,testFinal,n.trees = ntree)
predBoostFrame<-data.frame(Id=testFinal$Id,Hazard=predBoost)
write.table(predBoostFrame,file = paste0("predictions/attrImpBoosted/predBoost",depth,"_",ntree,"_",numAttr),quote = FALSE,sep = ",",row.names = FALSE)
predCv<-predict(modelBoost,cvData,n.trees = ntree)
g<-NormalizedGini(cvData$Hazard,predCv)
obs<-rbind(obs,data.frame(attrs=numAttr,gini=g))
print(paste0("iter=",iter, ",g=",g,",numAttr=",numAttr))
if(bestGini < g)
{
bestGini = g
bestNumAttr = numAttr
print(paste0("Best g=",g,",bestNumAttr=",numAttr))
}
}
library(gbm)
for(numAttr in seq(topStartAttr,32,2))
{
formulaStr = paste0("Hazard~",imp[1,"attr"])
for(i in 2:numAttr)
{
formulaStr<-paste0(formulaStr,"+",imp[i,"attr"])
}
print(formulaStr)
print(paste0("Ver 1 : Training for numAttr=",numAttr))
iter<-iter + 1
modelBoost<-gbm(as.formula(formulaStr),data = trainingData,distribution = "gaussian",n.trees = ntree,interaction.depth = depth)
predBoost<-predict(modelBoost,testFinal,n.trees = ntree)
predBoostFrame<-data.frame(Id=testFinal$Id,Hazard=predBoost)
write.table(predBoostFrame,file = paste0("predictions/attrImpBoosted/predBoost",depth,"_",ntree,"_",numAttr),quote = FALSE,sep = ",",row.names = FALSE)
predCv<-predict(modelBoost,cvData,n.trees = ntree)
g<-NormalizedGini(cvData$Hazard,predCv)
obs<-rbind(obs,data.frame(attrs=numAttr,gini=g))
print(paste0("iter=",iter, ",g=",g,",numAttr=",numAttr))
if(bestGini < g)
{
bestGini = g
bestNumAttr = numAttr
print(paste0("Best g=",g,",bestNumAttr=",numAttr))
}
}
set.seed(1988)
#"NormalizedGini" is the other half of the metric. This function does most of the work, though
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
#df
df$random = (1:nrow(df))/nrow(df)
#df
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
#print(df)
return(sum(df$Gini))
}
NormalizedGini <- function(solution, submission) {
SumModelGini(solution, submission) / SumModelGini(solution, solution)
}
library(leaps)
library(dplyr)
library(caret)
setwd("/Users/swapnil.jamthe/work/Kaggle/LMG")
data<-read.csv("train.csv")
data<-select(data,-(Id))
trainingIndex<-createDataPartition(y=data$Hazard,p=0.8,list=FALSE)
trainingData<-data[trainingIndex,]
cvData<-data[-trainingIndex,]
testFinal<-read.csv("test.csv")
obs<-data.frame(attrs=integer(0),gini=numeric(0))
bestGini = 0
bestNtrees = 0
iter = 0;
library(randomForest)
#modelRf<-randomForest(Hazard~.,data=trainingData,ntree=500,importance=TRUE)
#imp<-as.data.frame(importance(modelRf))
#imp$attr<-rownames(imp)
#imp<-imp[order(imp$`%IncMSE`,decreasing = TRUE),]
#write.table(imp,file = "predictions/attrImp/imp",quote = FALSE,sep = ",",row.names = FALSE)
imp<-read.csv("predictions/attrImpBoosted/imp")
imp<-imp[order(imp$X.IncMSE,decreasing=TRUE),]
obs<-data.frame(numAttr=integer(0),gini=numeric(0))
bestGini = 0
bestNumAttr = 0
iter = 0;
topStartAttr = 19
ntree = 15000
depth=25
library(gbm)
View(imp)
