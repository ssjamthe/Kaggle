{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/swapnil/anaconda2/lib/python27.zip', '/Users/swapnil/anaconda2/lib/python2.7', '/Users/swapnil/anaconda2/lib/python2.7/plat-darwin', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages', '/Users/swapnil/anaconda2/lib/python2.7/lib-tk', '/Users/swapnil/anaconda2/lib/python2.7/lib-old', '/Users/swapnil/anaconda2/lib/python2.7/lib-dynload', '/Users/swapnil/anaconda2/lib/python2.7/site-packages', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/Sphinx-1.5.1-py2.7.egg', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/aeosa', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/IPython/extensions', '/Users/swapnil/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "EMBEDDING_FILE = '/Users/swapnil/work/Kaggle/data/googleNewsEmbeddings/GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_FILE = '/Users/swapnil/work/Kaggle/data/quoraPairs/train.csv'\n",
    "EMBEDDING_DIM = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "                                            binary=True)\n",
    "\n",
    "train = pd.read_csv(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors of word2vec\n",
      "0    255027\n",
      "1    149263\n",
      "Name: is_duplicate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "print(train['is_duplicate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test print\n"
     ]
    }
   ],
   "source": [
    "PRINT_STATUS_ITER = 50000\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "print('Test print')\n",
    "buckets = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", str(text))\n",
    "    return text\n",
    "\n",
    "class PairScore:\n",
    "    def __init__(self,ind1,ind2,dist):\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "        self.dist = dist\n",
    "    \n",
    "    def __lt__(self,other):\n",
    "        return self.dist > other.dist\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self,vec1,vec2,ind1,ind2):\n",
    "        self.vec1 = vec1\n",
    "        self.vec2 = vec2\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "\n",
    "class DataInstance:\n",
    "    def __init__(self,dataId,pairs,label):\n",
    "        self.dataId = dataId\n",
    "        self.pairs = pairs\n",
    "        self.label = label\n",
    "    \n",
    "    \n",
    "def generateClosePairs(embedding1,embedding2,smallQuestion,bigQuestion):\n",
    "    listPairs = []\n",
    "    # Creating all possible pairs.\n",
    "    for i in range(0,embedding1.shape[0]):\n",
    "        for j in range(0,embedding2.shape[0]):\n",
    "            p = PairScore(i,j,sklearn.metrics.pairwise\n",
    "                          .cosine_similarity(embedding1[i].reshape(1,-1),embedding2[j].reshape(1,-1))[0])\n",
    "            listPairs.append(p)\n",
    "    \n",
    "    sortedPairs = sorted(listPairs)\n",
    "    \n",
    "    # Creating right pairs by considering each word only once.. \n",
    "    smallConsidered = [False for i in range(embedding1.shape[0])]\n",
    "    bigConsidered = [False for i in range(embedding2.shape[0])]\n",
    "    finalPairs = []\n",
    "    for pair in sortedPairs:\n",
    "        if smallConsidered[pair.ind1] == False and bigConsidered[pair.ind2] == False:\n",
    "            finalPair = Pair(embedding1[pair.ind1],embedding2[pair.ind2],pair.ind1,pair.ind2)\n",
    "            finalPairs.append(finalPair)\n",
    "            smallConsidered[pair.ind1] = True\n",
    "            bigConsidered[pair.ind2] = True\n",
    "            \n",
    "    for i,considered in enumerate(bigConsidered):\n",
    "        if considered == False:\n",
    "            finalPair = Pair(np.zeros(EMBEDDING_DIM),embedding2[i],-1,i)\n",
    "            finalPairs.append(finalPair)\n",
    "            \n",
    "    return finalPairs\n",
    "        \n",
    "    #for finalPair in finalPairs:\n",
    "     #   print(smallQuestion[finalPair.ind1] if finalPair.ind1!=-1 else \"-1\",\",\",bigQuestion[finalPair.ind2])\n",
    "    \n",
    "def processData(data,isTrain):\n",
    "    processedList = []\n",
    "    maxSeqLen = 0\n",
    "    print('Start of processData')\n",
    "    for ind,row in train.iterrows():\n",
    "        question1 = cleanText(row['question1'])\n",
    "        question2 = cleanText(row['question2'])\n",
    "        label = row['is_duplicate']\n",
    "        dataId = row['id']\n",
    "    \n",
    "        question1Splits = question1.lower().split()\n",
    "        question2Splits = question2.lower().split()\n",
    "\n",
    "        # Removing stopwords.\n",
    "        shortQuestion1 = [w for w in question1Splits if not w in stops]\n",
    "        shortQuestion2 = [w for w in question2Splits if not w in stops]\n",
    "\n",
    "        # Deciding short question and long question.\n",
    "        if len(shortQuestion1) < len(shortQuestion2):\n",
    "            smallQuestion = shortQuestion1\n",
    "            bigQuestion = shortQuestion2\n",
    "        else:\n",
    "            smallQuestion = shortQuestion2\n",
    "            bigQuestion = shortQuestion1\n",
    "        \n",
    "        smallEmbedding = np.zeros((len(smallQuestion), EMBEDDING_DIM))\n",
    "        bigEmbedding = np.zeros((len(bigQuestion), EMBEDDING_DIM))\n",
    "    \n",
    "        # Getting embedding vectors for each word.\n",
    "        wordsBelowScore = np.zeros(len(buckets))\n",
    "    \n",
    "        for i in range(0,len(smallQuestion)):\n",
    "            word = smallQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                smallEmbedding[i] = word2vec.word_vec(word)\n",
    "    \n",
    "        for i in range(0,len(bigQuestion)):\n",
    "            word = bigQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                bigEmbedding[i] = word2vec.word_vec(word)\n",
    "        \n",
    "        pairs = generateClosePairs(smallEmbedding,bigEmbedding,smallQuestion,bigQuestion)\n",
    "        processedList.append(DataInstance(dataId,pairs,label))\n",
    "        if len(pairs) > maxSeqLen:\n",
    "            maxSeqLen = len(pairs)\n",
    "        if ind % PRINT_STATUS_ITER == 0:\n",
    "            print('processData curr iter ',ind)\n",
    "            \n",
    "    return maxSeqLen,processedList\n",
    "\n",
    "def createDataset(processedList,maxSeqLen):\n",
    "    print('Start of createDataset')\n",
    "    dataset = np.zeros((len(processedList),maxSeqLen,EMBEDDING_DIM))\n",
    "    lenthSet = np.zeros(len(processedList))\n",
    "    labels = np.zeros(len(processedList))\n",
    "    dataIds = np.zeros(len(processedList))\n",
    "    for i,processed in enumerate(processedList):\n",
    "        pairs = processed.pairs\n",
    "        j = 0;\n",
    "        for pair in pairs:\n",
    "            diffSqr = np.square(np.subtract(pair.vec1,pair.vec2))\n",
    "            dataset[i,j,:] = diffSqr\n",
    "            j = j + 1;\n",
    "        while j < maxSeqLen:\n",
    "            dataset[i,j,:] = -1 * np.ones(EMBEDDING_DIM)\n",
    "            j = j + 1\n",
    "        lenthSet[i] = len(pairs)\n",
    "        labels[i] = processed.label\n",
    "        dataIds[i] = processed.dataId\n",
    "        if i % PRINT_STATUS_ITER == 0:\n",
    "            print('createDataset curr iter ',i)\n",
    "    return dataset,lenthSet,labels,dataIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of processData\n",
      "('processData curr iter ', 0)\n",
      "('processData curr iter ', 50000)\n",
      "('processData curr iter ', 100000)\n",
      "('processData curr iter ', 150000)\n",
      "('processData curr iter ', 200000)\n",
      "('processData curr iter ', 250000)\n",
      "('processData curr iter ', 300000)\n",
      "('processData curr iter ', 350000)\n",
      "('processData curr iter ', 400000)\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/train/trainProcessedList.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ed5b4664825c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainMaxSeqLen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainProcessedList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/train/trainProcessedList.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainProcessedList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/train/trainMaxSeqLen.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainMaxSeqLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/swapnil/anaconda2/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/train/trainProcessedList.npy'"
     ]
    }
   ],
   "source": [
    "#trainMaxSeqLen,trainProcessedList = processData(train,True)    \n",
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/train/trainProcessedList.npy',trainProcessedList)\n",
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/train/trainMaxSeqLen.npy',trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy',trainProcessedList)\n",
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy',trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of createDataset\n",
      "('createDataset curr iter ', 0)\n",
      "('createDataset curr iter ', 50000)\n"
     ]
    }
   ],
   "source": [
    "trainProcessedList = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy')\n",
    "trainMaxSeqLen = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy')\n",
    "\n",
    "trainDataset,trainLengthSet,trainLabels,trainDataIds = createDataset(processedList,maxSeqLen)\n",
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataset.npy',trainDataset)\n",
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLengthSet.npy',trainLengthSet)\n",
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLabels.npy',trainLabels)\n",
    "np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataIds.npy',trainDataIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'maxSeqLen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-02b8c5c389d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrnnCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbatchData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxSeqLen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mseqLengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maxSeqLen' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    batchData = tf.placeholder(tf.float32, [None, maxSeqLen, EMBEDDING_DIM])\n",
    "    batchSeqLength = tf.placeholder(tf.int32,[tf.shape(batchData)[0]])\n",
    "    \n",
    "    labels = tf.placeholder(tf.float32,[BATCH_SIZE])\n",
    "    logisticWeights = tf.Variable(tf.truncated_normal(\n",
    "      [EMBEDDING_DIM], stddev=0.1))\n",
    "    \n",
    "    def model(batchData,seqLength):\n",
    "        rnnCell = tf.contrib.rnn.BasicLSTMCell(100)\n",
    "        rnnOutputs, state = tf.nn.dynamic_rnn(rnnCell, batchData, dtype=tf.float32)\n",
    "        lastIndex = tf.range(BATCH_SIZE)*tf.shape(rnnOutputs)[1] + (seqLength - 1)\n",
    "        lastOutput = tf.gather(tf.reshape(rnnOutputs, [-1, state_size]), lastIndex)\n",
    "        bias = tf.placeholder(tf.float32,[1])\n",
    "        logits = tf.matmul(lastOutput,logisticWeights) + bias\n",
    "        return logits\n",
    "    \n",
    "    logits = model(batchData,batchSeqLength)\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), labels)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))   \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-58-7044db1fd25f>:27: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "[3 3]\n",
      "[2 3 5]\n",
      "[[[1 2 3 0 0]\n",
      "  [1 2 0 0 0]\n",
      "  [1 2 3 4 5]]\n",
      "\n",
      " [[1 2 3 0 0]\n",
      "  [1 2 0 0 0]\n",
      "  [1 2 3 4 5]]]\n",
      "[[1 2 3 4 5]\n",
      " [1 2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def last_relevant(output, length):\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    out_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    return relevant\n",
    "    #return output\n",
    "    \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    out = tf.constant([[[1,2,3,0,0],[1,2,0,0,0],[0,0,0,0,0]],[[1,2,3,0,0],[1,2,0,0,0],[1,2,3,4,5]]])\n",
    "    outShape = tf.shape(out)\n",
    "    lengthOuts = length(out)\n",
    "    lastOuts = last_relevant(out,lengthOuts)\n",
    "    #s = tf.shape(out)\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  lengthVal,outs,out,outShape = session.run([lengthOuts,lastOuts,out,outShape])\n",
    "  print(lengthVal)\n",
    "  print(outShape)\n",
    "  print(out)\n",
    "  print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    a = tf.constant([[5,6,7],[7,8,9]])\n",
    "    b = tf.Variable(2)\n",
    "    b.assign(2)\n",
    "    b = tf.add(a,b)\n",
    "    c = a*b\n",
    "    \n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  aout,bout,cout = session.run([a,b,c])\n",
    "  print(aout,bout,cout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "l1 = [1,2,3]\n",
    "print(type(l1))\n",
    "l2 = l1\n",
    "l2.append(4)\n",
    "print(l1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
