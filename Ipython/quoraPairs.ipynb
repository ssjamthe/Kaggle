{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/swapnil/anaconda2/lib/python27.zip', '/Users/swapnil/anaconda2/lib/python2.7', '/Users/swapnil/anaconda2/lib/python2.7/plat-darwin', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages', '/Users/swapnil/anaconda2/lib/python2.7/lib-tk', '/Users/swapnil/anaconda2/lib/python2.7/lib-old', '/Users/swapnil/anaconda2/lib/python2.7/lib-dynload', '/Users/swapnil/anaconda2/lib/python2.7/site-packages', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/Sphinx-1.5.1-py2.7.egg', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/aeosa', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/IPython/extensions', '/Users/swapnil/.ipython']\n",
      "('tf version ', '1.0.0')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "EMBEDDING_FILE = '/Users/swapnil/work/Kaggle/data/googleNewsEmbeddings/GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_FILE = '/Users/swapnil/work/Kaggle/data/quoraPairs/train.csv'\n",
    "TEST_FILE = '/Users/swapnil/work/Kaggle/data/quoraPairs/test.csv'\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print('tf version ',tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "                                            binary=True)\n",
    "\n",
    "trainFullData = pd.read_csv(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total rows ', 404290)\n",
      "('Valid rows ', 50535)\n",
      "('Train rows ', 353754)\n",
      "            id    qid1    qid2  \\\n",
      "159745  159745   54574  249310   \n",
      "96584    96584  142096   38552   \n",
      "116684  116684  190011  190012   \n",
      "206875  206875  223600  310419   \n",
      "\n",
      "                                                question1  \\\n",
      "159745       What causes people to feel tired in old age?   \n",
      "96584   What should we answer when a HR questions why ...   \n",
      "116684   Would you like to date a tomboy? Why or why not?   \n",
      "206875  Who do Republicans dislike more, Bernie Sander...   \n",
      "\n",
      "                                                question2  is_duplicate  \n",
      "159745     When do we really feel that we are in old age?             0  \n",
      "96584   What is the best answer for why should we hire...             1  \n",
      "116684  How likely is it for a guy to have relationshi...             0  \n",
      "206875  Are #NeverTrump republicans more likely to vot...             0  \n",
      "Found 3000000 word vectors of word2vec\n",
      "('train value counts ', 0    223157\n",
      "1    130597\n",
      "Name: is_duplicate, dtype: int64)\n",
      "('valid value counts ', 0    31870\n",
      "1    18665\n",
      "Name: is_duplicate, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print('Total rows ',trainFullData.shape[0])\n",
    "train = trainFullData.sample(frac=1.0,random_state=1988)\n",
    "valid = train.iloc[range(1,train.shape[0]/8),:]\n",
    "print('Valid rows ',valid.shape[0])\n",
    "train = train.iloc[range(train.shape[0]/8,train.shape[0]),:]\n",
    "print('Train rows ',train.shape[0])\n",
    "\n",
    "print(train.iloc[range(1,5),:])\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "print('train value counts ',train['is_duplicate'].value_counts())\n",
    "print('valid value counts ',valid['is_duplicate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test print\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 200\n",
    "PRINT_STATUS_ITER = 50000\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "print('Test print')\n",
    "buckets = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", str(text))\n",
    "    return text\n",
    "\n",
    "class PairScore:\n",
    "    def __init__(self,ind1,ind2,dist):\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "        self.dist = dist\n",
    "    \n",
    "    def __lt__(self,other):\n",
    "        return self.dist > other.dist\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self,vec1,vec2,ind1,ind2):\n",
    "        self.vec1 = vec1\n",
    "        self.vec2 = vec2\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "\n",
    "class DataInstance:\n",
    "    def __init__(self,dataId,pairs,label):\n",
    "        self.dataId = dataId\n",
    "        self.pairs = pairs\n",
    "        self.label = label\n",
    "    \n",
    "    \n",
    "def generateClosePairs(embedding1,embedding2,smallQuestion,bigQuestion):\n",
    "    listPairs = []\n",
    "    # Creating all possible pairs.\n",
    "    for i in range(0,embedding1.shape[0]):\n",
    "        for j in range(0,embedding2.shape[0]):\n",
    "            p = PairScore(i,j,sklearn.metrics.pairwise\n",
    "                          .cosine_similarity(embedding1[i].reshape(1,-1),embedding2[j].reshape(1,-1))[0])\n",
    "            listPairs.append(p)\n",
    "    \n",
    "    sortedPairs = sorted(listPairs)\n",
    "    \n",
    "    # Creating pairs by finding closest word to the word from shorter question.\n",
    "    smallConsidered = [False for i in range(embedding1.shape[0])]\n",
    "    bigConsidered = [False for i in range(embedding2.shape[0])]\n",
    "    finalPairs = []\n",
    "    for pair in sortedPairs:\n",
    "        if smallConsidered[pair.ind1] == False:\n",
    "            finalPair = Pair(embedding1[pair.ind1],embedding2[pair.ind2],pair.ind1,pair.ind2)\n",
    "            finalPairs.append(finalPair)\n",
    "            smallConsidered[pair.ind1] = True\n",
    "            bigConsidered[pair.ind2] = True\n",
    "            \n",
    "    for i,considered in enumerate(bigConsidered):\n",
    "        if considered == False:\n",
    "            finalPair = Pair(np.zeros(EMBEDDING_DIM),embedding2[i],-1,i)\n",
    "            finalPairs.append(finalPair)\n",
    "            \n",
    "    return finalPairs\n",
    "        \n",
    "    #for finalPair in finalPairs:\n",
    "     #   print(smallQuestion[finalPair.ind1] if finalPair.ind1!=-1 else \"-1\",\",\",bigQuestion[finalPair.ind2])\n",
    "    \n",
    "def processData(data,isTrain):\n",
    "    processedList = []\n",
    "    maxSeqLen = 0\n",
    "    print('Start of processData')\n",
    "    indRow = 1\n",
    "    for _,row in data.iterrows():\n",
    "        question1 = cleanText(row['question1'])\n",
    "        question2 = cleanText(row['question2'])\n",
    "        if isTrain == True:\n",
    "            label = row['is_duplicate']\n",
    "        else:\n",
    "            label = None\n",
    "          \n",
    "        if isTrain == True:\n",
    "            dataId = row['id']\n",
    "        else:\n",
    "            dataId = row['test_id']\n",
    "    \n",
    "        question1Splits = question1.lower().split()\n",
    "        question2Splits = question2.lower().split()\n",
    "\n",
    "        # Removing stopwords.\n",
    "        shortQuestion1 = [w for w in question1Splits if not w in stops]\n",
    "        shortQuestion2 = [w for w in question2Splits if not w in stops]\n",
    "\n",
    "        # Deciding short question and long question.\n",
    "        if len(shortQuestion1) < len(shortQuestion2):\n",
    "            smallQuestion = shortQuestion1\n",
    "            bigQuestion = shortQuestion2\n",
    "        else:\n",
    "            smallQuestion = shortQuestion2\n",
    "            bigQuestion = shortQuestion1\n",
    "        \n",
    "        smallEmbedding = np.zeros((len(smallQuestion), EMBEDDING_DIM))\n",
    "        bigEmbedding = np.zeros((len(bigQuestion), EMBEDDING_DIM))\n",
    "    \n",
    "        # Getting embedding vectors for each word.\n",
    "        wordsBelowScore = np.zeros(len(buckets))\n",
    "    \n",
    "        for i in range(0,len(smallQuestion)):\n",
    "            word = smallQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                smallEmbedding[i] = word2vec.word_vec(word)\n",
    "    \n",
    "        for i in range(0,len(bigQuestion)):\n",
    "            word = bigQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                bigEmbedding[i] = word2vec.word_vec(word)\n",
    "        \n",
    "        pairs = generateClosePairs(smallEmbedding,bigEmbedding,smallQuestion,bigQuestion)\n",
    "        processedList.append(DataInstance(dataId,pairs,label))\n",
    "        if len(pairs) > maxSeqLen:\n",
    "            maxSeqLen = len(pairs)\n",
    "        if indRow % PRINT_STATUS_ITER == 0:\n",
    "            print('processData curr iter ',indRow)\n",
    "            \n",
    "    return maxSeqLen,processedList\n",
    "\n",
    "def createDataset(processedList):\n",
    "    print('Start of createDataset')\n",
    "    dataset = np.zeros((len(processedList),MAX_SEQ_LEN,EMBEDDING_DIM))\n",
    "    lenthSet = np.zeros(len(processedList))\n",
    "    labels = np.zeros(len(processedList))\n",
    "    dataIds = np.zeros(len(processedList))\n",
    "    for i,processed in enumerate(processedList):\n",
    "        pairs = processed.pairs\n",
    "        j = 0;\n",
    "        for pair in pairs:\n",
    "            diffSqr = np.square(np.subtract(pair.vec1,pair.vec2))\n",
    "            dataset[i,j,:] = diffSqr\n",
    "            j = j + 1;\n",
    "        while j < MAX_SEQ_LEN:\n",
    "            dataset[i,j,:] = -1 * np.ones(EMBEDDING_DIM)\n",
    "            j = j + 1\n",
    "        lenthSet[i] = len(pairs)\n",
    "        labels[i] = processed.label\n",
    "        dataIds[i] = processed.dataId\n",
    "        if i % PRINT_STATUS_ITER == 0:\n",
    "            print('createDataset curr iter ',i)\n",
    "    return dataset,lenthSet,labels,dataIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainMaxSeqLen,trainProcessedList = processData(train,True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy',trainProcessedList)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy',trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainProcessedList = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy')\n",
    "#trainMaxSeqLen = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(trainProcessedList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainDataset,trainLengthSet,trainLabels,trainDataIds = createDataset(trainProcessedList,trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataset.npy',trainDataset)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLengthSet.npy',trainLengthSet)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLabels.npy',trainLabels)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataIds.npy',trainDataIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of processData\n",
      "Start of createDataset\n",
      "('createDataset curr iter ', 0)\n",
      "('createDataset curr iter ', 50000)\n"
     ]
    }
   ],
   "source": [
    "validSeqLen,processedValidList = processData(valid,True)\n",
    "validData,validSeqLen,validLabels,_ = createDataset(processedValidList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50535, 200, 300)\n"
     ]
    }
   ],
   "source": [
    "print(validData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "STATE_SIZE = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    batchDataTensor = tf.placeholder(tf.float32, [BATCH_SIZE, MAX_SEQ_LEN, EMBEDDING_DIM])\n",
    "    batchSeqLengthTensor = tf.placeholder(tf.int32,[BATCH_SIZE])\n",
    "    batchLabelsTensor = tf.placeholder(tf.float32,[BATCH_SIZE])\n",
    "    \n",
    "    validDataTensor = tf.placeholder(tf.float32,shape=validData.shape)\n",
    "    validSeqLengthTensor = tf.placeholder(tf.int32, shape=validSeqLen.shape)\n",
    "    validLabelsTensor = tf.placeholder(tf.float32,[validData.shape[0]])\n",
    "    \n",
    "    #logisticWeights = tf.Variable(tf.truncated_normal([STATE_SIZE,1], stddev=0.1))\n",
    "    #bias = tf.Variable(tf.truncated_normal([1], stddev=0.1))\n",
    "    \n",
    "    #testBatchData = tf.placeholder(tf.float32, [None, maxSeqLen, EMBEDDING_DIM])\n",
    "    #testBatchSeqLength = tf.placeholder(tf.int32,[tf.shape(batchData)[0]])\n",
    "    \n",
    "    \n",
    "    #init_state = tf.get_variable('init_state', [1, STATE_SIZE],\n",
    "                                 #initializer=tf.constant_initializer(0.0))\n",
    "    #init_state = tf.tile(init_state, [BATCH_SIZE, 1])\n",
    "    \n",
    "    #init_state = tf.get_variable('init_state', [BATCH_SIZE, STATE_SIZE],initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    init_state = tf.contrib.rnn.LSTMStateTuple(tf.zeros([BATCH_SIZE,STATE_SIZE]),tf.zeros([BATCH_SIZE,STATE_SIZE]))\n",
    "    \n",
    "    def getLogits(rnnOutputs,modelSeqLength):\n",
    "        logisticWeights = tf.get_variable('logisticWeights',initializer=tf.truncated_normal([STATE_SIZE,1], stddev=0.1))\n",
    "        bias = tf.get_variable('logisticBias',initializer=tf.truncated_normal([1], stddev=0.1))\n",
    "        lastIndex = tf.range(tf.shape(rnnOutputs)[0])*tf.shape(rnnOutputs)[1] + (modelSeqLength - 1)\n",
    "        lastOutput = tf.gather(tf.reshape(rnnOutputs, [-1, STATE_SIZE]), lastIndex)\n",
    "        logits = tf.matmul(lastOutput,logisticWeights) + bias\n",
    "        return logits\n",
    "    \n",
    "    def model(modelData,modelSeqLength):\n",
    "        rnnCell = tf.contrib.rnn.BasicLSTMCell(STATE_SIZE,state_is_tuple=True)\n",
    "        rnnOutputs, state = tf.nn.dynamic_rnn(rnnCell, modelData, dtype=tf.float32,initial_state=init_state)\n",
    "        return getLogits(rnnOutputs,modelSeqLength)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('train') as scope:\n",
    "        logits = model(batchDataTensor,batchSeqLengthTensor)\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=logits[:,0], labels=batchLabelsTensor))   \n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "        batchPreds = tf.nn.sigmoid(logits[:,0])\n",
    "        batchLogLoss = tf.losses.log_loss(batchLabelsTensor, batchPreds)\n",
    "        scope.reuse_variables()\n",
    "        rnnCell = tf.contrib.rnn.BasicLSTMCell(STATE_SIZE,state_is_tuple=True)\n",
    "        rnnOutputs, state = tf.nn.dynamic_rnn(rnnCell, validDataTensor, dtype=tf.float32)\n",
    "        validLogits = getLogits(rnnOutputs,validSeqLengthTensor)\n",
    "        #validLogits = model(validDataTensor,validSeqLengthTensor)\n",
    "        validPreds = tf.nn.sigmoid(validLogits[:,0])\n",
    "        validLogLoss = tf.losses.log_loss(validLabelsTensor, validPreds)\n",
    "    #testPreds = tf.nn.sigmoid(model(testBatchData,testBatchSeqLength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-c33e5f306ad6>:9: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Start of processData\n",
      "Start of createDataset\n",
      "('createDataset curr iter ', 0)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(preds,labels):\n",
    "    predClass = np.where(preds > 0.5,np.ones(len(preds)),np.zeros(len(preds)))\n",
    "    return float(sum(predClass == labels)) / len(preds)\n",
    "\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  saver = tf.train.Saver()\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * BATCH_SIZE) % (train.shape[0] - BATCH_SIZE)\n",
    "    batchDataFrame = train.iloc[range(offset,(offset + BATCH_SIZE)),:]\n",
    "    batchSeqLen,processedBatchList = processData(batchDataFrame,True)\n",
    "    batchData,batchSeqLen,batchLabels,_ = createDataset(processedBatchList)\n",
    "    \n",
    "    feed_dict = {batchDataTensor : batchData, batchSeqLengthTensor : batchSeqLen, validDataTensor:validData, \n",
    "                validSeqLengthTensor:validSeqLen, batchLabelsTensor:batchLabels, validLabelsTensor:validLabels}\n",
    "    _, loss, batchPreds,batchLogLoss,validLogLoss= session.run(\n",
    "      [optimizer, loss, batchPreds,batchLogLoss,validLogLoss], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, loss))\n",
    "      print('Minibatch accuracy: %.3f%%' % accuracy(batchPreds, batchLabels))\n",
    "      print('Minibatch log loss: %.3f%%' % batchLogLoss)\n",
    "      print('Validation accuracy: %.3f%%' % accuracy(validPreds.eval(), validLabels))\n",
    "      print('Validation log loss: %.3f%%' % validLogLoss)\n",
    "    \n",
    "    if (step % 50000 == 0):\n",
    "        saver.save(sess, '/Users/swapnil/work/Kaggle/data/quoraPairs/models/firstModel/model',global_step=step)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(batchDataFrame))\n",
    "graph = tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "with graph.as_default():\n",
    "    a = tf.constant([[5,6,7],[7,8,9]])\n",
    "    b = tf.Variable([[5,6,7],[7,8,9]])\n",
    "    c = a*b\n",
    "    r = tf.range(3)\n",
    "    d = a - 1\n",
    "    e = tf.reshape(a,[-1,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
