{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/swapnil/anaconda2/lib/python27.zip', '/Users/swapnil/anaconda2/lib/python2.7', '/Users/swapnil/anaconda2/lib/python2.7/plat-darwin', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages', '/Users/swapnil/anaconda2/lib/python2.7/lib-tk', '/Users/swapnil/anaconda2/lib/python2.7/lib-old', '/Users/swapnil/anaconda2/lib/python2.7/lib-dynload', '/Users/swapnil/anaconda2/lib/python2.7/site-packages', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/Sphinx-1.5.1-py2.7.egg', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/aeosa', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/IPython/extensions', '/Users/swapnil/.ipython']\n",
      "('tf version ', '1.0.0')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "EMBEDDING_FILE = '/Users/swapnil/work/Kaggle/data/googleNewsEmbeddings/GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_FILE = '/Users/swapnil/work/Kaggle/data/quoraPairs/train.csv'\n",
    "TEST_FILE = '/Users/swapnil/work/Kaggle/data/quoraPairs/test.csv'\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print('tf version ',tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "                                            binary=True)\n",
    "\n",
    "trainFullData = pd.read_csv(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total rows ', 404290)\n",
      "('Valid rows ', 5052)\n",
      "('Train rows ', 399237)\n",
      "            id    qid1    qid2  \\\n",
      "304110  304110   78351  367254   \n",
      "385709  385709  490187  408229   \n",
      "112546  112546  131684  184123   \n",
      "21116    21116   39783   39784   \n",
      "\n",
      "                                                question1  \\\n",
      "304110              How do I overcome my extreme anxiety?   \n",
      "385709                  How should I focus on my studies?   \n",
      "112546  What is the use of having flavors in the condoms?   \n",
      "21116                   What are the theories of poverty?   \n",
      "\n",
      "                                      question2  is_duplicate  \n",
      "304110     How do you cope with social anxiety?             1  \n",
      "385709                    How I focus on study?             1  \n",
      "112546    What is the use of flavoured condoms?             1  \n",
      "21116   Is telepathy real? What is the process?             0  \n",
      "Found 3000000 word vectors of word2vec\n",
      "('train value counts ', 0    251845\n",
      "1    147392\n",
      "Name: is_duplicate, dtype: int64)\n",
      "('valid value counts ', 0    3182\n",
      "1    1870\n",
      "Name: is_duplicate, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print('Total rows ',trainFullData.shape[0])\n",
    "train = trainFullData.sample(frac=1.0,random_state=1988)\n",
    "valid = train.iloc[range(1,train.shape[0]/80),:]\n",
    "print('Valid rows ',valid.shape[0])\n",
    "train = train.iloc[range(train.shape[0]/80,train.shape[0]),:]\n",
    "print('Train rows ',train.shape[0])\n",
    "\n",
    "print(train.iloc[range(1,5),:])\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "print('train value counts ',train['is_duplicate'].value_counts())\n",
    "print('valid value counts ',valid['is_duplicate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test print\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 200\n",
    "PRINT_STATUS_ITER = 50000\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "print('Test print')\n",
    "buckets = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", str(text))\n",
    "    return text\n",
    "\n",
    "class PairScore:\n",
    "    def __init__(self,ind1,ind2,dist):\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "        self.dist = dist\n",
    "    \n",
    "    def __lt__(self,other):\n",
    "        return self.dist > other.dist\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self,vec1,vec2,ind1,ind2):\n",
    "        self.vec1 = vec1\n",
    "        self.vec2 = vec2\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "\n",
    "class DataInstance:\n",
    "    def __init__(self,dataId,pairs,label):\n",
    "        self.dataId = dataId\n",
    "        self.pairs = pairs\n",
    "        self.label = label\n",
    "    \n",
    "    \n",
    "def generateClosePairs(embedding1,embedding2,smallQuestion,bigQuestion):\n",
    "    listPairs = []\n",
    "    # Creating all possible pairs.\n",
    "    for i in range(0,embedding1.shape[0]):\n",
    "        for j in range(0,embedding2.shape[0]):\n",
    "            p = PairScore(i,j,sklearn.metrics.pairwise\n",
    "                          .cosine_similarity(embedding1[i].reshape(1,-1),embedding2[j].reshape(1,-1))[0])\n",
    "            listPairs.append(p)\n",
    "    \n",
    "    sortedPairs = sorted(listPairs)\n",
    "    \n",
    "    # Creating pairs by finding closest word to the word from shorter question.\n",
    "    smallConsidered = [False for i in range(embedding1.shape[0])]\n",
    "    bigConsidered = [False for i in range(embedding2.shape[0])]\n",
    "    finalPairs = []\n",
    "    for pair in sortedPairs:\n",
    "        if smallConsidered[pair.ind1] == False:\n",
    "            finalPair = Pair(embedding1[pair.ind1],embedding2[pair.ind2],pair.ind1,pair.ind2)\n",
    "            finalPairs.append(finalPair)\n",
    "            smallConsidered[pair.ind1] = True\n",
    "            bigConsidered[pair.ind2] = True\n",
    "            \n",
    "    for i,considered in enumerate(bigConsidered):\n",
    "        if considered == False:\n",
    "            finalPair = Pair(np.zeros(EMBEDDING_DIM),embedding2[i],-1,i)\n",
    "            finalPairs.append(finalPair)\n",
    "            \n",
    "    return finalPairs\n",
    "        \n",
    "    #for finalPair in finalPairs:\n",
    "     #   print(smallQuestion[finalPair.ind1] if finalPair.ind1!=-1 else \"-1\",\",\",bigQuestion[finalPair.ind2])\n",
    "    \n",
    "def processData(data,isTrain):\n",
    "    processedList = []\n",
    "    maxSeqLen = 0\n",
    "    indRow = 1\n",
    "    for _,row in data.iterrows():\n",
    "        question1 = cleanText(row['question1'])\n",
    "        question2 = cleanText(row['question2'])\n",
    "        if isTrain == True:\n",
    "            label = row['is_duplicate']\n",
    "        else:\n",
    "            label = None\n",
    "          \n",
    "        if isTrain == True:\n",
    "            dataId = row['id']\n",
    "        else:\n",
    "            dataId = row['test_id']\n",
    "    \n",
    "        question1Splits = question1.lower().split()\n",
    "        question2Splits = question2.lower().split()\n",
    "\n",
    "        # Removing stopwords.\n",
    "        shortQuestion1 = [w for w in question1Splits if not w in stops]\n",
    "        shortQuestion2 = [w for w in question2Splits if not w in stops]\n",
    "\n",
    "        # Deciding short question and long question.\n",
    "        if len(shortQuestion1) < len(shortQuestion2):\n",
    "            smallQuestion = shortQuestion1\n",
    "            bigQuestion = shortQuestion2\n",
    "        else:\n",
    "            smallQuestion = shortQuestion2\n",
    "            bigQuestion = shortQuestion1\n",
    "        \n",
    "        smallEmbedding = np.zeros((len(smallQuestion), EMBEDDING_DIM))\n",
    "        bigEmbedding = np.zeros((len(bigQuestion), EMBEDDING_DIM))\n",
    "    \n",
    "        # Getting embedding vectors for each word.\n",
    "        wordsBelowScore = np.zeros(len(buckets))\n",
    "    \n",
    "        for i in range(0,len(smallQuestion)):\n",
    "            word = smallQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                smallEmbedding[i] = word2vec.word_vec(word)\n",
    "    \n",
    "        for i in range(0,len(bigQuestion)):\n",
    "            word = bigQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                bigEmbedding[i] = word2vec.word_vec(word)\n",
    "        \n",
    "        pairs = generateClosePairs(smallEmbedding,bigEmbedding,smallQuestion,bigQuestion)\n",
    "        processedList.append(DataInstance(dataId,pairs,label))\n",
    "        if len(pairs) > maxSeqLen:\n",
    "            maxSeqLen = len(pairs)\n",
    "            \n",
    "    return maxSeqLen,processedList\n",
    "\n",
    "def createDataset(processedList):\n",
    "    dataset = np.zeros((len(processedList),MAX_SEQ_LEN,EMBEDDING_DIM))\n",
    "    lenthSet = np.zeros(len(processedList))\n",
    "    labels = np.zeros(len(processedList))\n",
    "    dataIds = np.zeros(len(processedList))\n",
    "    for i,processed in enumerate(processedList):\n",
    "        pairs = processed.pairs\n",
    "        j = 0;\n",
    "        for pair in pairs:\n",
    "            diffSqr = np.square(np.subtract(pair.vec1,pair.vec2))\n",
    "            dataset[i,j,:] = diffSqr\n",
    "            j = j + 1;\n",
    "        while j < MAX_SEQ_LEN:\n",
    "            dataset[i,j,:] = -1 * np.ones(EMBEDDING_DIM)\n",
    "            j = j + 1\n",
    "        lenthSet[i] = len(pairs)\n",
    "        labels[i] = processed.label\n",
    "        dataIds[i] = processed.dataId\n",
    "    return dataset,lenthSet,labels,dataIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainMaxSeqLen,trainProcessedList = processData(train,True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy',trainProcessedList)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy',trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainProcessedList = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy')\n",
    "#trainMaxSeqLen = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(trainProcessedList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainDataset,trainLengthSet,trainLabels,trainDataIds = createDataset(trainProcessedList,trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataset.npy',trainDataset)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLengthSet.npy',trainLengthSet)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLabels.npy',trainLabels)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataIds.npy',trainDataIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of processData\n",
      "Start of createDataset\n",
      "('createDataset curr iter ', 0)\n"
     ]
    }
   ],
   "source": [
    "validSeqLen,processedValidList = processData(valid,True)\n",
    "validData,validSeqLen,validLabels,_ = createDataset(processedValidList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5052, 200, 300)\n"
     ]
    }
   ],
   "source": [
    "print(validData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "STATE_SIZE = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    batchDataTensor = tf.placeholder(tf.float32, [BATCH_SIZE, MAX_SEQ_LEN, EMBEDDING_DIM])\n",
    "    batchSeqLengthTensor = tf.placeholder(tf.int32,[BATCH_SIZE])\n",
    "    batchLabelsTensor = tf.placeholder(tf.float32,[BATCH_SIZE])\n",
    "    \n",
    "    validDataTensor = tf.placeholder(tf.float32,shape=validData.shape)\n",
    "    validSeqLengthTensor = tf.placeholder(tf.int32, shape=validSeqLen.shape)\n",
    "    validLabelsTensor = tf.placeholder(tf.float32,[validData.shape[0]])\n",
    "    \n",
    "    #logisticWeights = tf.Variable(tf.truncated_normal([STATE_SIZE,1], stddev=0.1))\n",
    "    #bias = tf.Variable(tf.truncated_normal([1], stddev=0.1))\n",
    "    \n",
    "    #testBatchData = tf.placeholder(tf.float32, [None, maxSeqLen, EMBEDDING_DIM])\n",
    "    #testBatchSeqLength = tf.placeholder(tf.int32,[tf.shape(batchData)[0]])\n",
    "    \n",
    "    \n",
    "    #init_state = tf.get_variable('init_state', [1, STATE_SIZE],\n",
    "                                 #initializer=tf.constant_initializer(0.0))\n",
    "    #init_state = tf.tile(init_state, [BATCH_SIZE, 1])\n",
    "    \n",
    "    #init_state = tf.get_variable('init_state', [BATCH_SIZE, STATE_SIZE],initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    init_state = tf.contrib.rnn.LSTMStateTuple(tf.zeros([BATCH_SIZE,STATE_SIZE]),tf.zeros([BATCH_SIZE,STATE_SIZE]))\n",
    "    \n",
    "    def getLogits(rnnOutputs,modelSeqLength):\n",
    "        logisticWeights = tf.get_variable('logisticWeights',initializer=tf.truncated_normal([STATE_SIZE,1], stddev=0.1))\n",
    "        bias = tf.get_variable('logisticBias',initializer=tf.truncated_normal([1], stddev=0.1))\n",
    "        lastIndex = tf.range(tf.shape(rnnOutputs)[0])*tf.shape(rnnOutputs)[1] + (modelSeqLength - 1)\n",
    "        lastOutput = tf.gather(tf.reshape(rnnOutputs, [-1, STATE_SIZE]), lastIndex)\n",
    "        logits = tf.matmul(lastOutput,logisticWeights) + bias\n",
    "        return logits\n",
    "    \n",
    "    def model(modelData,modelSeqLength):\n",
    "        rnnCell = tf.contrib.rnn.BasicLSTMCell(STATE_SIZE,state_is_tuple=True)\n",
    "        rnnOutputs, state = tf.nn.dynamic_rnn(rnnCell, modelData, dtype=tf.float32,initial_state=init_state)\n",
    "        return getLogits(rnnOutputs,modelSeqLength)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('train') as scope:\n",
    "        logits = model(batchDataTensor,batchSeqLengthTensor)\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=logits[:,0], labels=batchLabelsTensor))   \n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "        batchPreds = tf.nn.sigmoid(logits[:,0])\n",
    "        batchLogLoss = tf.losses.log_loss(batchLabelsTensor, batchPreds)\n",
    "        scope.reuse_variables()\n",
    "        rnnCell = tf.contrib.rnn.BasicLSTMCell(STATE_SIZE,state_is_tuple=True)\n",
    "        rnnOutputs, state = tf.nn.dynamic_rnn(rnnCell, validDataTensor, dtype=tf.float32)\n",
    "        validLogits = getLogits(rnnOutputs,validSeqLengthTensor)\n",
    "        #validLogits = model(validDataTensor,validSeqLengthTensor)\n",
    "        validPreds = tf.nn.sigmoid(validLogits[:,0])\n",
    "        validLogLoss = tf.losses.log_loss(validLabelsTensor, validPreds)\n",
    "    #testPreds = tf.nn.sigmoid(model(testBatchData,testBatchSeqLength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-37-9d781683624b>:9: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.678359\n",
      "Minibatch accuracy: 0.600%\n",
      "Minibatch log loss: 0.678%\n",
      "Validation accuracy: 0.630%\n",
      "Validation log loss: 0.673%\n",
      "Minibatch loss at step 50: 0.627589\n",
      "Minibatch accuracy: 0.700%\n",
      "Minibatch log loss: 0.628%\n",
      "Validation accuracy: 0.630%\n",
      "Validation log loss: 0.656%\n",
      "Minibatch loss at step 100: 0.669608\n",
      "Minibatch accuracy: 0.600%\n",
      "Minibatch log loss: 0.670%\n",
      "Validation accuracy: 0.630%\n",
      "Validation log loss: 0.652%\n",
      "Minibatch loss at step 150: 0.697492\n",
      "Minibatch accuracy: 0.540%\n",
      "Minibatch log loss: 0.697%\n",
      "Validation accuracy: 0.630%\n",
      "Validation log loss: 0.649%\n",
      "Minibatch loss at step 200: 0.593164\n",
      "Minibatch accuracy: 0.720%\n",
      "Minibatch log loss: 0.593%\n",
      "Validation accuracy: 0.630%\n",
      "Validation log loss: 0.647%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(preds,labels):\n",
    "    predClass = np.where(preds > 0.5,np.ones(len(preds)),np.zeros(len(preds)))\n",
    "    return float(sum(predClass == labels)) / len(preds)\n",
    "\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  saver = tf.train.Saver()\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * BATCH_SIZE) % (train.shape[0] - BATCH_SIZE)\n",
    "    batchDataFrame = train.iloc[range(offset,(offset + BATCH_SIZE)),:]\n",
    "    batchSeqLen,processedBatchList = processData(batchDataFrame,True)\n",
    "    batchData,batchSeqLen,batchLabels,_ = createDataset(processedBatchList)\n",
    "    \n",
    "    feed_dict = {batchDataTensor : batchData, batchSeqLengthTensor : batchSeqLen, validDataTensor:validData, \n",
    "                validSeqLengthTensor:validSeqLen, batchLabelsTensor:batchLabels, validLabelsTensor:validLabels}\n",
    "    _, lossVal, batchPredsVal,validPredsVal,batchLogLossVal,validLogLossVal= session.run(\n",
    "      [optimizer, loss, batchPreds,validPreds,batchLogLoss,validLogLoss], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, lossVal))\n",
    "      print('Minibatch accuracy: %.3f%%' % accuracy(batchPredsVal, batchLabels))\n",
    "      print('Minibatch log loss: %.3f%%' % batchLogLossVal)\n",
    "      print('Validation accuracy: %.3f%%' % accuracy(validPredsVal, validLabels))\n",
    "      print('Validation log loss: %.3f%%' % validLogLossVal)\n",
    "    \n",
    "    if (step % 50000 == 0):\n",
    "        saver.save(session, '/Users/swapnil/work/Kaggle/data/quoraPairs/models/firstModel/model',global_step=step)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "(11,)\n",
      "24256800128\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(sys.getsizeof(np.zeros(11)))\n",
    "print(np.shape(np.zeros(11)))\n",
    "print(sys.getsizeof(validData))\n",
    "\n",
    "print(type(batchDataFrame))\n",
    "graph = tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "with graph.as_default():\n",
    "    a = tf.constant([[5,6,7],[7,8,9]])\n",
    "    b = tf.Variable([[5,6,7],[7,8,9]])\n",
    "    c = a*b\n",
    "    r = tf.range(3)\n",
    "    d = a - 1\n",
    "    e = tf.reshape(a,[-1,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
