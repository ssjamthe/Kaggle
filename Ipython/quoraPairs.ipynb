{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/swapnil/anaconda2/lib/python27.zip', '/Users/swapnil/anaconda2/lib/python2.7', '/Users/swapnil/anaconda2/lib/python2.7/plat-darwin', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac', '/Users/swapnil/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages', '/Users/swapnil/anaconda2/lib/python2.7/lib-tk', '/Users/swapnil/anaconda2/lib/python2.7/lib-old', '/Users/swapnil/anaconda2/lib/python2.7/lib-dynload', '/Users/swapnil/anaconda2/lib/python2.7/site-packages', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/Sphinx-1.5.1-py2.7.egg', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/aeosa', '/Users/swapnil/anaconda2/lib/python2.7/site-packages/IPython/extensions', '/Users/swapnil/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "EMBEDDING_FILE = '/Users/swapnil/work/Kaggle/data/googleNewsEmbeddings/GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_FILE = '/Users/swapnil/work/Kaggle/data/quoraPairs/train.csv'\n",
    "TEST_FILE = '/Users/swapnil/work/Kaggle/data/quoraPairs/test.csv'\n",
    "EMBEDDING_DIM = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "                                            binary=True)\n",
    "\n",
    "train = pd.read_csv(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total rows ', 404290)\n",
      "('Valid rows ', 50535)\n",
      "('Train rows ', 353754)\n",
      "            id    qid1    qid2  \\\n",
      "87009    87009   83026   12516   \n",
      "146831  146831  231854  231855   \n",
      "263492  263492  380056   18860   \n",
      "120830  120830  195921  195922   \n",
      "\n",
      "                                                question1  \\\n",
      "87009                 Why did MS Dhoni give up captaincy?   \n",
      "146831  I'm 22 and have a steady job, but I'm not happ...   \n",
      "263492                  Why does the Earth have blue sky?   \n",
      "120830           How do I distance myself from my family?   \n",
      "\n",
      "                                                question2  is_duplicate  \n",
      "87009     Why did MS Dhoni resign as ODI and T20 Captain?             1  \n",
      "146831  Does a charged capacitor weigh more than a dis...             0  \n",
      "263492                      Why is the sky blue on Earth?             1  \n",
      "120830  How can I distance myself from my family and b...             1  \n",
      "Found 3000000 word vectors of word2vec\n",
      "('train value counts ', 0    223108\n",
      "1    130646\n",
      "Name: is_duplicate, dtype: int64)\n",
      "('valid value counts ', 0    31918\n",
      "1    18617\n",
      "Name: is_duplicate, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "train = train.sample(frac=1.0)\n",
    "print('Total rows ',train.shape[0])\n",
    "valid = train.iloc[range(1,train.shape[0]/8),:]\n",
    "print('Valid rows ',valid.shape[0])\n",
    "train = train.iloc[range(train.shape[0]/8,train.shape[0]),:]\n",
    "print('Train rows ',train.shape[0])\n",
    "\n",
    "print(train.iloc[range(1,5),:])\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "print('train value counts ',train['is_duplicate'].value_counts())\n",
    "print('valid value counts ',valid['is_duplicate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test print\n"
     ]
    }
   ],
   "source": [
    "PRINT_STATUS_ITER = 50000\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "print('Test print')\n",
    "buckets = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", str(text))\n",
    "    return text\n",
    "\n",
    "class PairScore:\n",
    "    def __init__(self,ind1,ind2,dist):\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "        self.dist = dist\n",
    "    \n",
    "    def __lt__(self,other):\n",
    "        return self.dist > other.dist\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self,vec1,vec2,ind1,ind2):\n",
    "        self.vec1 = vec1\n",
    "        self.vec2 = vec2\n",
    "        self.ind1 = ind1\n",
    "        self.ind2 = ind2\n",
    "\n",
    "class DataInstance:\n",
    "    def __init__(self,dataId,pairs,label):\n",
    "        self.dataId = dataId\n",
    "        self.pairs = pairs\n",
    "        self.label = label\n",
    "    \n",
    "    \n",
    "def generateClosePairs(embedding1,embedding2,smallQuestion,bigQuestion):\n",
    "    listPairs = []\n",
    "    # Creating all possible pairs.\n",
    "    for i in range(0,embedding1.shape[0]):\n",
    "        for j in range(0,embedding2.shape[0]):\n",
    "            p = PairScore(i,j,sklearn.metrics.pairwise\n",
    "                          .cosine_similarity(embedding1[i].reshape(1,-1),embedding2[j].reshape(1,-1))[0])\n",
    "            listPairs.append(p)\n",
    "    \n",
    "    sortedPairs = sorted(listPairs)\n",
    "    \n",
    "    # Creating pairs by finding closest word to the word from shorter question.\n",
    "    smallConsidered = [False for i in range(embedding1.shape[0])]\n",
    "    bigConsidered = [False for i in range(embedding2.shape[0])]\n",
    "    finalPairs = []\n",
    "    for pair in sortedPairs:\n",
    "        if smallConsidered[pair.ind1] == False:\n",
    "            finalPair = Pair(embedding1[pair.ind1],embedding2[pair.ind2],pair.ind1,pair.ind2)\n",
    "            finalPairs.append(finalPair)\n",
    "            smallConsidered[pair.ind1] = True\n",
    "            bigConsidered[pair.ind2] = True\n",
    "            \n",
    "    for i,considered in enumerate(bigConsidered):\n",
    "        if considered == False:\n",
    "            finalPair = Pair(np.zeros(EMBEDDING_DIM),embedding2[i],-1,i)\n",
    "            finalPairs.append(finalPair)\n",
    "            \n",
    "    return finalPairs\n",
    "        \n",
    "    #for finalPair in finalPairs:\n",
    "     #   print(smallQuestion[finalPair.ind1] if finalPair.ind1!=-1 else \"-1\",\",\",bigQuestion[finalPair.ind2])\n",
    "    \n",
    "def processData(data,isTrain):\n",
    "    processedList = []\n",
    "    maxSeqLen = 0\n",
    "    print('Start of processData')\n",
    "    for ind,row in train.iterrows():\n",
    "        question1 = cleanText(row['question1'])\n",
    "        question2 = cleanText(row['question2'])\n",
    "        if isTrain == True:\n",
    "            label = row['is_duplicate']\n",
    "        else:\n",
    "            label = None\n",
    "          \n",
    "        if isTrain == True:\n",
    "            dataId = row['id']\n",
    "        else:\n",
    "            dataId = row['test_id']\n",
    "    \n",
    "        question1Splits = question1.lower().split()\n",
    "        question2Splits = question2.lower().split()\n",
    "\n",
    "        # Removing stopwords.\n",
    "        shortQuestion1 = [w for w in question1Splits if not w in stops]\n",
    "        shortQuestion2 = [w for w in question2Splits if not w in stops]\n",
    "\n",
    "        # Deciding short question and long question.\n",
    "        if len(shortQuestion1) < len(shortQuestion2):\n",
    "            smallQuestion = shortQuestion1\n",
    "            bigQuestion = shortQuestion2\n",
    "        else:\n",
    "            smallQuestion = shortQuestion2\n",
    "            bigQuestion = shortQuestion1\n",
    "        \n",
    "        smallEmbedding = np.zeros((len(smallQuestion), EMBEDDING_DIM))\n",
    "        bigEmbedding = np.zeros((len(bigQuestion), EMBEDDING_DIM))\n",
    "    \n",
    "        # Getting embedding vectors for each word.\n",
    "        wordsBelowScore = np.zeros(len(buckets))\n",
    "    \n",
    "        for i in range(0,len(smallQuestion)):\n",
    "            word = smallQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                smallEmbedding[i] = word2vec.word_vec(word)\n",
    "    \n",
    "        for i in range(0,len(bigQuestion)):\n",
    "            word = bigQuestion[i]\n",
    "            if word in word2vec.vocab:\n",
    "                bigEmbedding[i] = word2vec.word_vec(word)\n",
    "        \n",
    "        pairs = generateClosePairs(smallEmbedding,bigEmbedding,smallQuestion,bigQuestion)\n",
    "        processedList.append(DataInstance(dataId,pairs,label))\n",
    "        if len(pairs) > maxSeqLen:\n",
    "            maxSeqLen = len(pairs)\n",
    "        if ind % PRINT_STATUS_ITER == 0:\n",
    "            print('processData curr iter ',ind)\n",
    "            \n",
    "    return maxSeqLen,processedList\n",
    "\n",
    "def createDataset(processedList,maxSeqLen):\n",
    "    print('Start of createDataset')\n",
    "    dataset = np.zeros((len(processedList),maxSeqLen,EMBEDDING_DIM))\n",
    "    lenthSet = np.zeros(len(processedList))\n",
    "    labels = np.zeros(len(processedList))\n",
    "    dataIds = np.zeros(len(processedList))\n",
    "    for i,processed in enumerate(processedList):\n",
    "        pairs = processed.pairs\n",
    "        j = 0;\n",
    "        for pair in pairs:\n",
    "            diffSqr = np.square(np.subtract(pair.vec1,pair.vec2))\n",
    "            dataset[i,j,:] = diffSqr\n",
    "            j = j + 1;\n",
    "        while j < maxSeqLen:\n",
    "            dataset[i,j,:] = -1 * np.ones(EMBEDDING_DIM)\n",
    "            j = j + 1\n",
    "        lenthSet[i] = len(pairs)\n",
    "        labels[i] = processed.label\n",
    "        dataIds[i] = processed.dataId\n",
    "        if i % PRINT_STATUS_ITER == 0:\n",
    "            print('createDataset curr iter ',i)\n",
    "    return dataset,lenthSet,labels,dataIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of processData\n",
      "('processData curr iter ', 0)\n",
      "('processData curr iter ', 50000)\n",
      "('processData curr iter ', 100000)\n",
      "('processData curr iter ', 150000)\n",
      "('processData curr iter ', 200000)\n",
      "('processData curr iter ', 250000)\n",
      "('processData curr iter ', 300000)\n",
      "('processData curr iter ', 350000)\n",
      "('processData curr iter ', 400000)\n"
     ]
    }
   ],
   "source": [
    "#trainMaxSeqLen,trainProcessedList = processData(train,True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy',trainProcessedList)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy',trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of createDataset\n",
      "('createDataset curr iter ', 0)\n",
      "('createDataset curr iter ', 50000)\n",
      "('createDataset curr iter ', 100000)\n"
     ]
    }
   ],
   "source": [
    "#trainProcessedList = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainProcessedList.npy')\n",
    "#trainMaxSeqLen = np.load('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainMaxSeqLen.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(trainProcessedList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainDataset,trainLengthSet,trainLabels,trainDataIds = createDataset(trainProcessedList,trainMaxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataset.npy',trainDataset)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLengthSet.npy',trainLengthSet)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainLabels.npy',trainLabels)\n",
    "#np.save('/Users/swapnil/work/Kaggle/data/quoraPairs/processedData/trainDataIds.npy',trainDataIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of processData\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "graph = tf.Graph()\n",
    "\n",
    "validDataFrame = valid.iloc[range(0,(valid.shape[0])),:]\n",
    "validSeqLen,processedValidList = processData(validDataFrame,True)\n",
    "validData,validSeqLen,validLabels,_ = createDataset(processedValidList,validSeqLen)\n",
    "\n",
    "with graph.as_default():\n",
    "    batchData = tf.placeholder(tf.float32, [None, maxSeqLen, EMBEDDING_DIM])\n",
    "    batchSeqLength = tf.placeholder(tf.int32,[tf.shape(batchData)[0]])\n",
    "    \n",
    "    validData = tf.constant(validData)\n",
    "    validSeqLength = tf.constant(validSeqLen)\n",
    "    \n",
    "    labels = tf.placeholder(tf.float32,[BATCH_SIZE])\n",
    "    \n",
    "    #testBatchData = tf.placeholder(tf.float32, [None, maxSeqLen, EMBEDDING_DIM])\n",
    "    #testBatchSeqLength = tf.placeholder(tf.int32,[tf.shape(batchData)[0]])\n",
    "    \n",
    "    logisticWeights = tf.Variable(tf.truncated_normal(\n",
    "      [EMBEDDING_DIM], stddev=0.1))\n",
    "    \n",
    "    def model(modelData,modelSeqLength):\n",
    "        rnnCell = tf.contrib.rnn.BasicLSTMCell(100)\n",
    "        rnnOutputs, state = tf.nn.dynamic_rnn(rnnCell, modelData, dtype=tf.float32)\n",
    "        lastIndex = tf.range(tf.shape(rnnOutputs)[0])*tf.shape(rnnOutputs)[1] + (modelSeqLength - 1)\n",
    "        lastOutput = tf.gather(tf.reshape(rnnOutputs, [-1, state_size]), lastIndex)\n",
    "        bias = tf.placeholder(tf.float32,[1])\n",
    "        logits = tf.matmul(lastOutput,logisticWeights) + bias\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    logits = model(batchData,batchSeqLength)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))   \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    batchPreds = tf.nn.sigmoid(logits)\n",
    "    validPreds = tf.nn.sigmoid(model(validData,validSeqLength))\n",
    "    #testPreds = tf.nn.sigmoid(model(testBatchData,testBatchSeqLength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(preds,labels):\n",
    "    predClass = np.where(preds > 0.5,np.ones(len(preds)),np.zeros(len(preds)))\n",
    "    return float(sum(predClass == labels)) / len(preds)\n",
    "\n",
    "\n",
    "num_steps = 1001\n",
    "batch_size = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batchDataFrame = train.iloc[range(offset,(offset + batch_size)),:]\n",
    "    batchSeqLen,processedBatchList = processData(batchDataFrame,True)\n",
    "    batchData,batchSeqLen,batchLabels,_ = createDataset(processedBatchList,batchSeqLen)\n",
    "    \n",
    "    feed_dict = {batchData : batchData, batchSeqLength : batchSeqLen, validData:validData, \n",
    "                 validSeqLength:validSeqLen, labels:batchLabels}\n",
    "    _, loss, batchPreds= session.run(\n",
    "      [optimizer, loss, batchPreds], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, loss))\n",
    "      print('Minibatch accuracy: %.2f%%' % accuracy(batchPreds, batchLabels))\n",
    "      print('Minibatch log loss: %.2f%%' % tf.losses.log_loss(batchLabels, batchPreds))\n",
    "      print('Validation accuracy: %.2f%%' % accuracy(validPreds.eval(), validLabels))\n",
    "      print('Validation log loss: %.2f%%' % tf.losses.log_loss(validPreds.eval(), validLabels))\n",
    "    \n",
    "    if (step % 50000 == 0):\n",
    "        saver.save(sess, '/Users/swapnil/work/Kaggle/data/quoraPairs/models/firstModel/model',global_step=step)\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
